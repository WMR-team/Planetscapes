<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Benchmarks - Planetscapes Dataset</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <!-- 导航栏 -->
    <nav class="navbar">
      <div class="container">
        <div class="nav-brand"><a href="index.html">Planetscapes Dataset</a></div>
        <ul class="nav-menu">
          <li><a href="overview.html">Overview</a></li>
          <li><a href="examples.html">Examples</a></li>
          <li><a href="download.html">Download</a></li>
          <li><a href="benchmarks.html" class="active">Benchmarks</a></li>
          <li><a href="contact.html">Contact</a></li>
        </ul>
      </div>
    </nav>

    <!-- Benchmarks 内容 -->
    <section class="section">
      <div class="container">
        <h1>Benchmark Suite</h1>
        <p>
          The Planetscapes benchmark suite evaluates semantic, instance and
          panoptic segmentation methods on the dataset. Below are the primary
          evaluation metrics and placeholders for results and leaderboards.
        </p>

        <div class="benchmarks-overview">
          <h2>Pixel-Level Semantic Labeling Task</h2>
          <p>The Planetscapes task involves predicting a per-pixel semantic labeling of the image without considering higher-level object instance or boundary information.</p>
          <h3>Metrics</h3>
          <p>
            To access performance, we rely on the standard Jaccard Index, commonly know as the Pascal VOC intersection-over-union metric IoU = TP / (TP+FP+FN), 
            where TP, FP, and FN are the numbers of true positive, false positive, and false negative pixels, respectively, determined over the whole test set. 
            Owning tow the two semantic granularities, i.e. classes and categories, we report two separate mean performance scores: IoU_{category} and IoU_{class}.
            In either case, pixels labeled as void do not contribute to the score.

            It is well-known that the global IoU measure is biased toward object instances that cover a large image area. In street scenes with their strong scale variation this can be problematic. 
            Specifically for traffic participants, which are the key classes in our scenario, we aim to evaluate how well the individual instances in the scene are represented in the labeling. 
            To address this, we additionally evaluate the semantic labeling using an instance-level intersection-over-union metric iIoU = iTP / (iTP + FP + iFN), 
            where iTP, iFN are the true positive and false negative pixels weighted by the ratio of the average instance size to the size of the respective ground truth instance. 
            However, in contrast to the standard IoU measure, iTP and iFN are computed by weighting the contribution of each pixel by the ratio of the class' average instance size to the size of the respective ground truth instance.
            It is important to note here that unlike the instance-level task below, we assume that the methods only yield a standard per-pixel semantic class labeling as output. Therefore, the false positive pixels are not associated with any instance and 
            thus do not require normalization. The final scores, iIoU_{category} and iIoU_{class}, are computed as the means for the two semantic granularities.
          </p>
          <!-- <ul>
            <li><strong>mIoU</strong> — mean Intersection over Union for semantic segmentation</li>
            <li><strong>Pixel Accuracy</strong> — overall pixel-wise accuracy</li>
            <li><strong>AP / AP50</strong> — average precision for instance segmentation</li>
            <li><strong>PQ</strong> — panoptic quality for panoptic segmentation</li>
          </ul> -->

          <h3>Results</h3>
          <p>
            Leaderboard and detailed per-class scores will be provided here. For now,
            include your evaluation scripts and upload benchmark results to the repository.
          </p>

          <!-- 可替换为真实表格或外部 leaderboard 嵌入 -->
          <div class="results-placeholder">
            <p><em>Placeholder: benchmark tables and charts will appear here.</em></p>
          </div>

          <div class="table-card">
            <p class="table-note"><em>Note: The numbers below are placeholder/demo results (not official).</em></p>

            <div class="table-wrapper">
              <table class="benchmark-table">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>Backbone</th>
                    <th class="num">mIoU</th>
                    <th class="num">Pixel Acc</th>
                    <th class="num">AP</th>
                    <th class="num">AP50</th>
                    <th class="num">PQ</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Baseline-Seg</td>
                    <td>ResNet-50</td>
                    <td class="num">0.412</td>
                    <td class="num">0.892</td>
                    <td class="num">0.238</td>
                    <td class="num">0.421</td>
                    <td class="num">0.301</td>
                  </tr>
                  <tr>
                    <td>Transformer-Seg</td>
                    <td>Swin-T</td>
                    <td class="num">0.468</td>
                    <td class="num">0.906</td>
                    <td class="num">0.271</td>
                    <td class="num">0.459</td>
                    <td class="num">0.337</td>
                  </tr>
                  <tr>
                    <td>Instance-Plus</td>
                    <td>ResNet-101</td>
                    <td class="num">0.451</td>
                    <td class="num">0.901</td>
                    <td class="num">0.294</td>
                    <td class="num">0.487</td>
                    <td class="num">0.329</td>
                  </tr>
                  <tr>
                    <td><strong>Ours (Demo)</strong></td>
                    <td><strong>ViT-B</strong></td>
                    <td class="num"><strong>0.502</strong></td>
                    <td class="num"><strong>0.917</strong></td>
                    <td class="num"><strong>0.315</strong></td>
                    <td class="num"><strong>0.512</strong></td>
                    <td class="num"><strong>0.361</strong></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <!-- <h3>How to evaluate</h3>
          <ol>
            <li>Download the validation/test split from the Download page.</li>
            <li>Run the provided evaluation script (see repository).</li>
            <li>Submit results to the leaderboard or contact the maintainers for inclusion.</li>
          </ol> -->

        </div>
      </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
      <div class="container">
        <p>&copy; 2025 Planetscapes Dataset Project. All rights reserved.</p>
        <p>Contact: zhouryhit@gmail.com</p>
      </div>
    </footer>

    <script src="script.js"></script>
  </body>
</html>
